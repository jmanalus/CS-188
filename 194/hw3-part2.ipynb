{
 "metadata": {
  "name": "hw3-part2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "CS194-16: Introduction to Data Science\n\n__Name:__ *Jonathan Manalus*\n\n__Student ID:__ *22037129*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Homework 3\n\n# Part 2: Predicting Movie Ratings At Scale\n\nIn the first part of this assignment, you explored how to recommend movies to a user using a small version of the movies and ratings datasets.  In this part of the assignment, you'll first explore how to tune the parallelism of an application running on a cluster.  Then, you'll use your findings to run your code from the first part on a cluster of machines and using a larger dataset.  You'll use the cluster and larger dataset to predict what movies to recommend to yourself.  For many parts of this assignment, you should be able to use the exact code that you wrote in part 1!\n\nAs mentioned during the lab and in part 1, think carefully before calling `collect()` on any datasets.  When you're using a small, local dataset, calling `collect()` and then using Python to analyze the data locally will work fine, but this will not work when you're using a large dataset that doesn't fit in memory on one machine.  Solutions that call `collect()` and do local analysis that could have been done with Spark will not receive full credit.\n\nAs in the first part, we have created a [FAQ](#FAQ) at the bottom of this page to help with common problems you run into.  If you run into a problem, please check the FAQ before posting on Piazza!"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Exercise 0: Setup\n\nAs in the lab (and unlike part 1 of the homework), we've already created a `SparkContext` for you that's available with the `sc` variable name.  The code below prints out the URLs of the master UI and the UI for your application."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# We've setup the notebook so that the hostname of the master is saved\n# as CLUSTER_URL.\nmaster_ui_address = \"\".join(CLUSTER_URL.split(\"//\")[1].split(\":\")[0])\nprint \"Master UI located at http://%s:8080\" % master_ui_address\n\napplication_ui_address = \"http://\" + sc.appName + \":4040\"\nprint \"Application UI located at %s\" % application_ui_address",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Master UI located at http://ec2-54-85-120-154.compute-1.amazonaws.com:8080\nApplication UI located at http://ec2-54-85-142-77.compute-1.amazonaws.com:4040\n"
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "a) Use the `SparkContext` to create an RDD for each of the ratings and movies datasets, which are located at `/movielens/large/ratings.dat` and `/movielens/large/movies.dat`, respectively (as in the lab).  Recall that each entry in the ratings dataset is formatted as UserID::MovieID::Rating::Timestamp and each entry in the movies dataset is formatted as MovieID::Title::Genres.  Count the entries in each dataset to ensure you've correctly created the RDD; there should be 10000054 entries in the ratings dataset and 10681 entries in the movies dataset."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_ratings_tuple(entry):\n    items = entry.split(\"::\")\n    return int(items[0]), int(items[1]), float(items[2]), int(items[3])\nratings = sc.textFile(\"/movielens/large/ratings.dat\").map(get_ratings_tuple)\n\ndef get_movie_tuple(entry):\n    items = entry.split(\"::\")\n    return int(items[0]), items[1], items[2].split(\"|\")\nmovies = sc.textFile(\"/movielens/large/movies.dat\").map(get_movie_tuple)\n\nratings_count = ratings.count()\nmovies_count = movies.count()\nprint \"%s ratings and %s movies in the datasets\" % (ratings_count, movies_count)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "10000054 ratings and 10681 movies in the datasets\n"
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "b) As you did in part 1, use the least significant digit of the rating timestamp to separate 60% of the data into a training set, 20% into a validation set, and the remaining 20% into a test set."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "### YOUR CODE HERE\ntraining = ratings.filter(lambda x: x[3] % 10 < 6)\nvalidation = ratings.filter(lambda x: x[3] % 10 >= 6 and x[3] % 10 < 8)\ntest = ratings.filter(lambda x: x[3] % 10 >= 8)\nprint training.take(10)\nprint \"Training: %s, validation: %s, test: %s\" % (training.count(), validation.count(), test.count())",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[(1, 185, 5.0, 838983525), (1, 231, 5.0, 838983392), (1, 292, 5.0, 838983421), (1, 316, 5.0, 838983392), (1, 329, 5.0, 838983392), (1, 355, 5.0, 838984474), (1, 356, 5.0, 838983653), (1, 362, 5.0, 838984885), (1, 377, 5.0, 838983834), (1, 420, 5.0, 838983834)]\nTraining: 6002473, validation: 1999675, test: 1997906"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n"
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Now the datasets are 10x bigger, so the training set should have about 6 million entries, the validation set should have about 2 million entries, and the test set should have about 2 million entries."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Exercise 1: Understanding Parallelism\n\nBefore analyzing the movies datasets at scale, let's learn a little about how to optimize the performance.  In the lab, we saw that the number of partitions that a dataset is broken into can have a big affect on the response time of jobs run on that dataset.  In this part of the assignment, you'll determine the optimal number of partitions to break datasets into.\n\na) In the next part of this exercise, you'll break the ratings dataset into different numbers of partitions, and then call `count()`.  First, answer some questions to understand how this will work.\n\n> __How does the number of tasks relate to the number of partitions?__ *The number of partitions elates to the number of task in that the greater the number of partitions the \nsmaller the number of task that runs on each processor. *\n\nb)\n> __When you call `count()`, what does each task do?__ *When count is called, each task adds up the amount of entries on the partition and returns a single value. The accumulated value. It is noteworthy to state that if there are 10 partitions, then 10 ten count() task are launched when count is called on the dataset or RDD.*\n\nc)\n> __How do you expect the time to count the entries in the `ratings` dataset to vary with the number of partitions?__ *It is expected that the time to count the  entries in the ratings dataset will decrease with an increasing number of partitions. Conversely, the time to count the entries will increase with a decrease in the number of partitions. A directly proportional relationship is expected between the time to count the entries and the number of partitions.*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "d) Create an RDD for the `ratings` dataset with different numbers of partitions, and call `count()` on the resulting datasets.  Using Spark and [matplotlib](http://matplotlib.org/index.html), make a graph showing the response time of `count()` as a function of the number of partitions.  You're responsible for choosing the number of partitions to use to produce a meaningful graph of how `count()`'s runtime depends on the number of partitions. \n\n`sc.textFile` will never allow you to use fewer than 2 partitions for the `ratings` dataset, even if you pass 1 as the number of partitions.  This is because the underlying dataset is stored in HDFS (Hadoop Distributed Filesystem) as two files.  As a result, you don't need to measure the time for `count()` using just 1 partition."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import matplotlib.pyplot as plot\n# Magic command to make matplotlib and ipython play nicely together.\n%matplotlib inline\nimport atexit\nfrom time import clock\n\ndef get_ratings_tuple(entry):\n    items = entry.split(\"::\")\n    return int(items[0]), int(items[1]), float(items[2]), int(items[3])\nratings = sc.textFile(\"/movielens/large/ratings.dat\",27).map(get_ratings_tuple)\n \nratings_count = ratings.count()\nprint \"%s ratings in the datasets\" % (ratings_count)\n\nwidth = 0.3\npartitions_x = [2,5,8,10,13,15,17,19,20,22,25,27,30,35,36,37,40]\n\ncomputation_times = [59.2,27.2,29.3,23.6,26.5,24.4,24.5,23.8,21.4,23.5,22.4,23.2,21,26.4,35.6,61,61] \nplot.bar(partitions_x,computation_times, width)\n\n\n### YOUR CODE HERE",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "10000054 ratings in the datasets\n"
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": "<Container object of 17 artists>"
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFYZJREFUeJzt3X9s1GcBx/HPF8FsceuAXn8shYh242fvaKfM1gxyLDYd\nx4hkMgvYkjkwxjgR/iLTaDtIWGCZWxeMMc6ZyBmxOiVm7ZpQtxOnFprwo1WirlCSlsT2StfRUjqa\n9vEP5BhQ7lfv+r2nvF/Jkutdv899+lz58Nyz75dzjDFGAABrzXA7AABgcihyALAcRQ4AlqPIAcBy\nFDkAWI4iBwDLRS3yf//73yopKYn898ADD+i1115Tf3+/ysvL5fP5VFFRoYGBganKCwC4hRPveeTj\n4+MqKCjQ8ePHtX//fhUWFmrHjh169dVX1dnZqbq6unRnBQBMIO6tlebmZj300EOaP3++GhsbVV1d\nLUmqqqpSQ0ND2gICAKKLu8gPHTqkTZs2SZLC4bCys7MlSR6PR729velJBwCIKa6tlatXr6qgoEBn\nzpxRTk6OsrKydOnSpcjjt34NAJg6M+P5prfffluf+9znlJOTI0nKyclRX1+fPB6PwuGwcnNzJzzu\noYce0tmzZ1OXFgCmucLCQnV0dCR0TFxbK7/+9a8j2yqSFAgEFAwGJUnBYFCBQGDC486ePStjTEb9\nV1NT43oGMk2fTJmai0z2Zkpm8RtzRX758mU1NzfrZz/7WeS+F154QZWVlXrjjTeUn5+v+vr6hJ8Y\nAJAaMYv8U5/6lPr6+m66b+7cuTpy5EjaQgEA4nfXXdnp9/vdjnAbMsUnEzNJmZmLTPHJxEzJiPuC\noKQGdxylcXgAmHaS6c27bkUOANMNRQ4AlqPIAcByFDkAWI4iBwDLUeQAYDmKHAAsR5EDgOUocgCw\nHEUOAJajyAHAchQ5AFiOIgcAy1HkAGA5ihwALEeRA4DlKHIAsBxFDgCWo8gBwHIUOQBYjiIHAMvF\nLPKBgQE9/fTTWr58uZYsWaKWlhb19/ervLxcPp9PFRUVGhgYmIqsAIAJxCzyb3zjG3rqqad0+vRp\n/fOf/9TSpUtVU1OjtWvXqq2tTWvWrFFNTc1UZAVwl8rKmivHcZSVNdeKcaeaY4wxd3rw4sWLKi0t\n1fvvv3/T/YWFhTp+/Liys7PV19en0tJSdXR03D644yjK8AAQF8dxJBlJqe2UdI07Gcn0ZtQV+fvv\nv6+cnBx99atfVVFRkbZs2aLBwUGFw2FlZ2dLkjwej3p7e5NPDQCYlJnRHhwfH1dra6vq6uq0YsUK\n7dixQ3v27EnoCWprayO3/X6//H5/MjkBYFoKhUIKhUKTGiPq1kpXV5dWrlyp8+fPS5Lee+897d69\nW+fOnVNLS4s8Ho/C4bDKysrYWgGQNmytRBd1a2X+/PnyeDz6z3/+I0lqbm7WkiVLtGbNGgWDQUlS\nMBhUIBBIMjIAYLKirsgl6fTp09q2bZuGh4f16U9/Wr/61a9kjFFlZaV6enqUn5+v+vp6zZ49+/bB\nJ7Eiz8qaq8HBD3T//XN06VJ/UmMAmB5Ykcc4JlaRT8ZkijwTJxiAOyjy6LiyEwAsR5EDgOUocgCw\nHEUOAJajyAHAchQ5AFiOIgcAy1HkAGA5ihwALEeRA4DlKHIAsBxFDgCWo8gBwHIUOQBYjiIHAMtR\n5ABgOYocACxHkQOA5ShyALAcRQ4AlqPIAcByFDkAWI4iBwDLzYz1DQsWLFBWVpY+8YlPaNasWTp+\n/Lj6+/tVWVmpnp4ePfjgg/rNb36j2bNnT0VeAMAtYq7IHcdRKBTSyZMndfz4cUlSTU2N1q5dq7a2\nNq1Zs0Y1NTVpDwoAmFhcWyvGmJu+bmxsVHV1tSSpqqpKDQ0NqU8GAIhLXCvy8vJy+Xw+HThwQJIU\nDoeVnZ0tSfJ4POrt7U1vSgDAHcXcI29paVFubq7C4bCeeOIJLV68OKEnqK2tjdz2+/3y+/2JZgSA\naSsUCikUCk1qDMfcum8SxYsvvihJev3113Xs2DF5PB6Fw2GVlZWpo6Pj9sEd57ZtmbiDOY4kIyn5\nMQBMD+nqg0zsmWR6M+rWyvDwsIaHhyVJly9fVlNTk5YtW6ZAIKBgMChJCgaDCgQCSUYGAExW1BV5\nZ2en1q9fL8dxNDw8rI0bN2r37t03nX6Yn5+v+vr6CU8/ZEUOIBVYkcc4JpGtlURR5ABSgSKPjis7\nAcByFDkAWI4iBwDLUeQAYDmKHAAsR5EDgOUocgCwHEUOAJajyAHAchQ5AFiOIgcAy1HkAGA5ihwA\nLEeRA4DlKHIAsBxFDgCWo8gBwHIUOQBYjiIHAMtR5ABgOYocACxHkQOA5ShyALBcXEU+NjamkpIS\nrVu3TpLU2dmpsrIyeb1ebdy4UaOjo2kNCQC4s7iKvK6uTkuXLpXjOJKk7du3a9euXWpvb1d+fr4O\nHDiQ1pAAgDuLWeTd3d1qbGzUtm3bZIzR2NiYWlpatH79eklSVVWVGhoa0h4UADCxmEW+c+dOvfTS\nS5ox49q39vb2yuPxRB4vKChQd3d3+hICAKKaGe3Bt956S7m5uSopKVEoFJIkGWMSeoLa2trIbb/f\nL7/fn2hGAJi2QqFQpF+T5Zgozfy9731PBw8e1MyZMzUyMqJLly7pqaeeUlNTk8LhsCSptbVVzz//\nvJqbm28f3HESLv6PHysZScmPAWB6SFcfZGLPJNObUbdW9u7dq66uLnV2durQoUN6/PHHdfDgQZWW\nlurw4cOSpGAwqEAgkHxqAMCkJHQe+fWzVl577TXt27dPXq9XPT09+s53vpOWcACA2KJurUx6cLZW\nAKQAWyvRcWUnAFiOIgcAy1HkAGA5ihwALEeRA4DlKHIAsBxFDgCWo8gBwHIUOQBYjiIH4LqsrLly\nHEeO4ygra67bcazDJfoAXHfjz7s00Z95LtGPjhU5AFiOIgcAy1HkAGA5ihwALEeRA4DlKHIAsBxF\nDgCWo8gBwHIUOQBYjiIHAMtR5ABgOYocACwXtchHRka0YsUKlZSUaOHChdq5c6ckqbOzU2VlZfJ6\nvdq4caNGR0enJCwA4HZRi/yee+7R0aNHdfLkSZ05c0Z///vf9e6772r79u3atWuX2tvblZ+frwMH\nDkxVXgDALWJurdx7772SpKtXr2psbEy5ublqaWnR+vXrJUlVVVVqaGhIb0oAwB3FLPLx8XEVFxcr\nLy9Pq1ev1pw5c+TxeCKPFxQUqLu7O60hAQB3NjPWN8yYMUOnTp3Shx9+qIqKChUXFyf0BLW1tZHb\nfr9ffr8/0YwAMG2FQiGFQqFJjZHQJwTt2bNHjuOorq5O4XBYktTa2qrnn39ezc3Ntw/OJwQBiAOf\nEHRDyj8h6OLFixocHJQkXblyRUeOHFFxcbFKS0t1+PBhSVIwGFQgEEgy8vRx/TMH+bxBAFMt6oq8\nvb1dW7ZskTFGIyMj2rx5s374wx+qs7NTmzdv1tDQkJYtW6aDBw9q1qxZtw9+F63IbcsLZBJW5Dck\n05t8+HKK2JYXyCQU+Q18+DIA3IUocgCwHEUOAJajyAHAcnddkXOaIIDp5q47a8W2cYG7AWet3MBZ\nK7jJ9XcfvAMBpjeKfBobHPxA11Yb5v+3AfewrZk+Mf/RLABIhesLi8FBx+0o0w4rcgCwHEUOAJaj\nyAHAchQ5AFiOIseEJnOGAWcnAFOLC4IyfNzJiHWRRXzHJvFvI2fgXMB90X4vuCDoBi4IgvVYzQOJ\no8gzAOV1w41zjVN7ARNXuWI6o8gzQLrKCzdk6lWu/CWOVODKTsBFXO2IVGBFDkwSq2q4jRU5MEms\nquE2VuQAYDmKHAAsF7XIu7q6tGrVKnm9Xi1atEj79++XJPX396u8vFw+n08VFRUaGBiYkrC4u3EK\nITCxqFd29vT0KBwOq6ioSENDQ3rkkUf029/+Vq+//roKCwu1Y8cOvfrqq+rs7FRdXd3tg3NlZ4Zk\nUsJju/XzTObqv/jGTfzY+MfOnNc+K2uuBgc/0P33z9GlS/0pG3cyuLIzPim/sjMvL09FRUWSpPvu\nu08+n08XLlxQY2OjqqurJUlVVVVqaGhIMjKAdODahLtL3Hvk58+fV2trqx577DGFw2FlZ2dLkjwe\nj3p7e9MWEAAQXVynHw4NDWnDhg2qq6tTVlZWQk9QW1sbue33++X3+xM6HgCms1AopFAoNKkxYv7r\nh6Ojo3ryySf1xBNPaOfOnZKkwsJCHTt2TB6PR+FwWGVlZero6Lh9cPbIMySTEh6bPfL4ZfZrn1l7\nv+yRx5byPXJjjLZu3aqlS5dGSlySAoGAgsGgJCkYDCoQCCQRFwCQClFX5O+9955WrVoln8/3/7+5\npBdffFGPPvqoKisr1dPTo/z8fNXX12v27Nm3D86KPEMyKeGxWZHHL7Nf+8xaabIijy2Z3uSDJTJg\nXIo8vmMpcvfHnQyKPD58sAQA3IUocgCwHEUOAJajyAHAchQ5AFiOIgcAy1HkAGA5ihwALEeRA4Dl\nKHIAsBxFDgCWo8gBwHIUOQBYjiIHAMtR5ABgOYocACxHkQOA5ShyALAcRQ4AlqPIAcByFDkAWI4i\nBwDLUeQAYLmYRf7ss88qLy9PXq83cl9/f7/Ky8vl8/lUUVGhgYGBtIYEANxZzCL/+te/rqamppvu\nq6mp0dq1a9XW1qY1a9aopqYmbQEBANHFLPKVK1dqzpw5N93X2Nio6upqSVJVVZUaGhrSkw4AEFNS\ne+ThcFjZ2dmSJI/Ho97e3pSGAgDEb2a6n6C2tjZy2+/3y+/3p/spAcAaoVBIoVBoUmM4xhgT65vO\nnz+vdevWqb29XZJUWFioY8eOyePxKBwOq6ysTB0dHbcP7jiKY/iJgzmOJCMp+TFsGTf9mZTw2G79\nPNGOTc3Pk/ix8Y+dia99asedjMm8tnfbPCWaJamtlUAgoGAwKEkKBoMKBALJDAMASIGYK/JNmzbp\nz3/+s/r6+pSXl6fdu3fry1/+siorK9XT06P8/HzV19dr9uzZtw/OijxDMinhsVmRxy+zX/vMWmmy\nIo8tmd6Ma2slWRR5pmRSwmNT5PHL7Nc+swqKIo9tyrZWAACZgyIHAMtR5ABgOYocACxHkQOA5Shy\nALAcRQ4AlqPIAcByFDkAWI4iBwDLUeQAYDmKHAAsR5EDgOUocgCwHEUOAJajyAHAchQ5AFiOIgcA\ny1HkAGA5ihwALEeRA4DlKHIAsNykirypqUler1dLly7Vvn37UpUJAJCApIv8o48+0re+9S01NTWp\nra1Nv/vd73Ty5MlUZgMAxCHpIj927JiWLVumgoICzZw5U5WVlWpoaEhlNgBAHJIu8u7ubs2fPz/y\n9bx589Td3Z2SUACA+CVd5I7jpDIHACBJM5M9cN68eerq6op83dXVddMKXZIKCwsnWfjXjk39XxqZ\nOG56MyU3tls/T7RjJ//zJJ8r9tiZ+Npn1qJrMq/t3TFPhYWFCR/jGGNMMk82MjKixYsX669//aty\nc3P1xS9+UT/96U/1yCOPJDMcACBJSa/I77nnHv3kJz9RRUWFxsfHVV1dTYkDgAuSXpEDADJDWq7s\nzMQLhRYsWCCfz6eSkhI9+uijruV49tlnlZeXJ6/XG7mvv79f5eXl8vl8qqio0MDAgOuZamtrNW/e\nPJWUlKikpERNTU1Tmqmrq0urVq2S1+vVokWLtH//fknuztWdMrk5VyMjI1qxYoVKSkq0cOFC7dy5\nU5LU2dmpsrIyeb1ebdy4UaOjo1OWKVquZ555Rp/97Gcjc9XW1jaluSRpbGxMJSUlWrdunST352qi\nTAnPk0mxkZERs2DBAtPd3W1GR0fN5z//eXPixIlUP03CFixYYC5evOh2DHP06FFz4sQJU1RUFLnv\nueeeM6+88ooxxphXXnnFbN++3fVMtbW15uWXX57SHB/33//+17S3txtjjBkcHDQPP/ywOXXqlKtz\ndadMbs/V8PCwMcaY0dFR84UvfMG888475sknnzR/+MMfjDHGfPe73zU/+tGPMiLXM888Y958880p\nz/JxL7/8stm8ebNZt26dMcZkxFzdminReUr5ijyTLxQyGbCLtHLlSs2ZM+em+xobG1VdXS1Jqqqq\nmvL5miiT5O585eXlqaioSJJ03333yefz6cKFC67O1Z0ySe7O1b333itJunr1qsbGxpSbm6uWlhat\nX79ekju/U3fKJbk7V93d3WpsbNS2bdtkjNHY2Jjrc3VrpusSmaeUF3mmXijkOE7kLfmBAwfcjnOT\ncDis7OxsSZLH41Fvb6/Lia758Y9/rCVLlqiqqkr9/f2u5Th//rxaW1v12GOPZcxcXc+0cuVKSe7O\n1fj4uIqLi5WXl6fVq1drzpw58ng8kccLCgpc+TN4a65ly5ZJkr7//e9ryZIleu655/TRRx9Naaad\nO3fqpZde0owZ16qvt7fX9bm6NdN1icxTyos8U87FvFVLS4tOnDihP/3pT/rFL36h5uZmtyNltG9/\n+9s6e/aszpw5o8LCQm3fvt2VHENDQ9qwYYPq6uqUlZXlSoZbDQ0N6emnn1ZdXZ3uv/9+1+dqxowZ\nOnXqlLq7u3X06FGFQqEpff47mSjXvn379K9//UunT5/WlStXtGfPninL89Zbbyk3N1clJSWR1a7b\n79InyiQp4XlKeZHHc6GQG66/rcvJydGGDRvU2trqcqIbcnJy1NfXJ+na6vx6Vjd5PB45jiPHcfTN\nb37TlfkaHR3VV77yFX3ta1+LvPV1e66uZ9q8eXMkUybMlSQ98MADWrt2rc6dOxeZI+nau+R58+a5\nkunjuVpaWiKv1yc/+Ult3bp1Sufqb3/7m/74xz/qM5/5jDZt2qR33nlHu3btcnWuJsq0ZcuWhOcp\n5UW+YsUK/eMf/9CFCxc0Ojqq+vp6rVmzJtVPk5Dh4WENDw9Lki5fvqympqbI27xMEAgEFAwGJUnB\nYFCBQMDlRLppy+LNN9+c8vkyxmjr1q1aunRp5IwHyd25ulMmN+fq4sWLGhwclCRduXJFR44cUXFx\nsUpLS3X48GFJ7vxOTZTL6/VG5soYo9///vdTOld79+5VV1eXOjs7dejQIT3++OM6ePCgq3M1UaZf\n/vKXic9T6v6/6w2NjY1m2bJlZsmSJWbv3r3peIqEnDt3zvh8PrN8+XLz8MMPmx/84AeuZdm4caN5\n8MEHzaxZs8y8efPMG2+8YS5evGi+9KUvGa/Xa8rLy80HH3zgaqaf//znpqqqyvh8PrN48WJTUVFh\nuru7pzTTX/7yF+M4jlm+fLkpLi42xcXF5u2333Z1ribK1NjY6OpctbW1meLiYrN8+XKzaNEi88IL\nLxhjrv3Ol5aWmqKiIlNZWWmuXr06ZZmi5Vq9erVZvny5WbhwoamsrDQffvjhlOa6LhQKRc4QcXuu\nrnv33XcjmRKdJy4IAgDL8VFvAGA5ihwALEeRA4DlKHIAsBxFDgCWo8gBwHIUOQBYjiIHAMv9D8LB\n6wlqy/PDAAAAAElFTkSuQmCC\n",
       "text": "<matplotlib.figure.Figure at 0x37614d0>"
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "e)\n>__Explain the shape of the plot.  How does this differ from the hypothesis you provided in part (a)?  Can you explain the difference?__ *The Shape of the plot is that of a histogram that follows a gaussian distribution. The time taken to compute the count value was high for a small number of partitions. The value of the time  decreases up to a maximum value for the number of partitions after which the time starts to increase as the number of partitions increase. This differs from the hypothesis stated in part(a) in that whereas the actual plot shows  a decrease in the time up to the optimal number of partitions then an increase there after, the hypothesis was that there would be a directly proportional relationship between the two variables (number of partitions and the time taken to compute the count).*\n\nf)\n>__What is the optimal number of partitions for this dataset and why?__ *35 is the optimal number of partitions, since it took the least amount of time. Other possiable options are 11 and 20, but 35 partitions was the fastest of all the options. "
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Exercise 2: Adding Your Rankings\n\nThe ultimate goal of this assignment is to predict what movies to recommend to yourself.  In order to do that, you'll first need to add ratings for yourself to the `ratings` dataset.\n\na) First, generate a unique user ID for yourself.  Find the highest user ID in the `ratings` dataset and use 1 + this ID as your user ID. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "sorted_ratings = ratings.map(lambda x: (x[0], (x[1],x[2],x[3]))).sortByKey(False).first() \nhighest_existing_user_id = sorted_ratings[0]\nmy_user_id = highest_existing_user_id + 1\nprint \"My user ID is %s\" % my_user_id",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "My user ID is 71568\n"
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "b) Next, you need to create a RDD with some ratings for yourself.  Create a new RDD `my_ratings_RDD` with at least 10 movie ratings.  Each entry should be formatted as `(my_user_id, movieID, rating)` (i.e., each entry should be formatted in the same way as the `training` RDD).  As in the original dataset, ratings should be between 1 and 5 (inclusive).\n\nTo help you provide ratings for yourself, we've included some code below to list the names and movie IDs of the 50 most-rated movies (we did this in the lab; the code below is replicated from the lab).  Select 10 movies that you've seen from this list and use those (along with your own ratings for those movies) to produce the `my_ratings_RDD`.  If you haven't seen at least 10 of these movies, you can increase the parameter passed to `take()` until there are 10 movies that you have seen (or you can also guess what your rating would be for movies you haven't seen)."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Create a dataset of (movieID, number of ratings) pairs.\nratings_per_movie = ratings.map(lambda x: (x[1], 1)).reduceByKey(lambda x,y: x+y)\n\n# Join average_ratings with movies to get a dataset with movie names and average ratings.\nratings_with_names = movies.map(lambda x: (x[0], x[1])).join(ratings_per_movie)\n\n# map transforms ratings_with_names into an RDD where the key is the number of ratings\n# and the value is a 2-item tuple with (movie name, number of ratings). We reformat the\n# dataset in this way so that we can use sortByKey to get the most-rated movies.\nsorted_ratings = ratings_with_names.map(lambda x: (x[1][1], (x[1][0], x[0]))).sortByKey(False)\nprint \"Most rated movies:\"\nfor ratings_tuple in sorted_ratings.take(50):\n    print ratings_tuple",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Most rated movies:\n(34864, (u'Pulp Fiction (1994)', 296))"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n(34457, (u'Forrest Gump (1994)', 356))\n(33668, (u'Silence of the Lambs, The (1991)', 593))\n(32631, (u'Jurassic Park (1993)', 480))\n(31126, (u'Shawshank Redemption, The (1994)', 318))\n(29154, (u'Braveheart (1995)', 110))\n(28951, (u'Fugitive, The (1993)', 457))\n(28948, (u'Terminator 2: Judgment Day (1991)', 589))\n(28566, (u'Star Wars: Episode IV - A New Hope (a.k.a. Star Wars) (1977)', 260))\n(27035, (u'Apollo 13 (1995)', 150))\n(26996, (u'Batman (1989)', 592))\n(26449, (u'Toy Story (1995)', 1))\n(26042, (u'Independence Day (a.k.a. ID4) (1996)', 780))\n(25912, (u'Dances with Wolves (1990)', 590))\n(25777, (u\"Schindler's List (1993)\", 527))\n(25381, (u'True Lies (1994)', 380))\n(25098, (u'Star Wars: Episode VI - Return of the Jedi (1983)', 1210))\n(24397, (u'12 Monkeys (Twelve Monkeys) (1995)', 32))\n(24037, (u'Usual Suspects, The (1995)', 50))\n(23794, (u'Fargo (1996)', 608))\n(23748, (u'Speed (1994)', 377))\n(23531, (u'Aladdin (1992)', 588))\n(23229, (u'Matrix, The (1999)', 2571))\n(23091, (u'Star Wars: Episode V - The Empire Strikes Back (1980)', 1196))\n(22521, (u'Seven (a.k.a. Se7en) (1995)', 47))\n(22120, (u'American Beauty (1999)', 2858))\n(21803, (u'Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)', 1198))\n(21247, (u'Back to the Future (1985)', 1270))\n(21085, (u'Mission: Impossible (1996)', 648))\n(21014, (u'Ace Ventura: Pet Detective (1994)', 344))\n(20972, (u'Lion King, The (1994)', 364))\n(19878, (u'Beauty and the Beast (1991)', 595))\n(19814, (u'Godfather, The (1972)', 858))\n(19603, (u'Die Hard: With a Vengeance (1995)', 165))\n(19400, (u'Sixth Sense, The (1999)', 2762))\n(19326, (u'Batman Forever (1995)', 153))\n(19229, (u'Pretty Woman (1990)', 597))\n(19214, (u'Mrs. Doubtfire (1993)', 500))\n(19036, (u'Mask, The (1994)', 367))\n(18925, (u'Stargate (1994)', 316))\n(18924, (u'Saving Private Ryan (1998)', 2028))\n(18900, (u'Babe (1995)', 34))\n(18801, (u'Twister (1996)', 736))\n(18523, (u'Men in Black (1997)', 1580))\n(18327, (u'Clear and Present Danger (1994)', 349))\n(17964, (u'Rock, The (1996)', 733))\n(17851, (u'Dumb & Dumber (1994)', 231))\n(17611, (u'E.T. the Extra-Terrestrial (1982)', 1097))\n(17519, (u'Terminator, The (1984)', 1240))\n(17194, (u'Ghost (1990)', 587))\n"
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "my_ratings_RDD = sc.parallelize([(my_user_id,34864,3.0),(my_user_id,19814,2.0),(my_user_id,19326,5.0),(my_user_id,19036,4.0),(my_user_id,18523,5.0),(my_user_id,17519,4.0),(my_user_id,19603,3.0),(my_user_id,23229,5.0),(my_user_id,26996,3.0),(my_user_id,21085,4.0)])\n\n# Remember that in general, you shouldn't use collect()!\n# We use collect here because we know that the RDD with your\n# ratings is small.\nprint \"My movie ratings: \", my_ratings_RDD.collect()",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "My movie ratings:  [(71568, 34864, 3.0), (71568, 19814, 2.0), (71568, 19326, 5.0), (71568, 19036, 4.0), (71568, 18523, 5.0), (71568, 17519, 4.0), (71568, 19603, 3.0), (71568, 23229, 5.0), (71568, 26996, 3.0), (71568, 21085, 4.0)]\n"
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "c) Now that you have ratings for yourself, you need to add your ratings to the `training` dataset so that the model you train will incorporate your preferences.  Spark's `union` method combines two RDDs (see documentation [here](http://spark.apache.org/docs/0.9.0/api/pyspark/pyspark.rdd.RDD-class.html#union)); use `union` to create a new training dataset that includes your ratings and the data in the original training dataset."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "training_with_my_ratings = training.union(my_ratings_RDD)\nprint (\"The training dataset now has %s more entries than the original training dataset\" % \n       (training_with_my_ratings.count() - training.count()))",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "The training dataset now has 10 more entries than the original training dataset\n"
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The new training dataset should have at least 10 more entries than it had originally."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Exercise 3: Building a Model\n\na) In this exercise, you'll run your code from part 1 on the larger dataset.  First, we need to think a bit more carefully about how we do things to ensure using the larger dataset doesn't take too long on the cluster.  Use Spark's `repartition(numPartitions)` function (which re-partitions the dataset into the specified number of partitions) to break the `test`, `training_with_my_ratings`, and `validation` datasets into the optimal number of partitions that you found in part 1."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "optimal = 35\ntest.repartition(optimal)\ntraining_with_my_ratings.repartition(optimal)\nvalidation.repartition(optimal)\n\nprint test.take(5)\nprint validation.take(5)\nprint training_with_my_ratings.take(5)\nprint test.count()\n\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[(1, 466, 5.0, 838984679), (1, 520, 5.0, 838984679), (1, 539, 5.0, 838984068), (1, 586, 5.0, 838984068), (1, 588, 5.0, 838983339)]\n[(1, 122, 5.0, 838985046), (1, 364, 5.0, 838983707), (1, 370, 5.0, 838984596), (2, 110, 5.0, 868245777), (2, 1391, 3.0, 868246006)]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n[(1, 185, 5.0, 838983525), (1, 231, 5.0, 838983392), (1, 292, 5.0, 838983421), (1, 316, 5.0, 838983392), (1, 329, 5.0, 838983392)]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n1997906"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n"
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "b) As you did in part 1, train models with ranks of 4, 8, 12, and 16 using the `training_with_my_ratings` dataset (remember that you should use this dataset, which includes your `ratings`, instead of the original `training` dataset). Predict the ratings for the `validation` dataset, and use the `compute_error` function you wrote in part 1 to compute the error.  Which rank produces the best model, based on the error on the `validation` dataset?\n\nNow that you're running on a cluster, you'll want to think carefully about what datasets you cache in memory.  Remember that Spark only saves datasets in memory if you explicitly call `cache()` on the dataset, like we did in the lab.  You can use Spark's UI (located at port 4040 on the machine where this notebook is running) to see what datasets are cached in memory (you may want to use the `setName()` function to set a name for RDDs so they can easily be identified in the UI).  If you accidentally cache a dataset that you don't need, you can call `unpersist()` on the RDD to drop it from the cache."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from pyspark.mllib.recommendation import ALS\nimport math\n#function to compute mean square error\ndef compute_error(predicted, actual):\n    \"\"\" Compute the root mean squared error between predicted and actual.\n    \n    Params:\n      predicted: An RDD of predicted ratings for each movie and each user where each entry is in the form (user, movie, rating).\n      actual: An RDD of actual ratings where each entry is in the form (user, movie, rating).\n    \"\"\"\n    # Make each RDD in the format ((user, movie), rating) so we can easily join them together.\n    predicted_reformatted = predicted.map(lambda x: ((x[0], x[1]), x[2]))\n    actual_reformatted = actual.map(lambda x: ((x[0], x[1]), x[2]))\n    predicted_and_actual = predicted_reformatted.join(actual_reformatted)\n    squared_errors = predicted_and_actual.map(lambda x: (x[1][1] - x[1][0])**2)\n    total_error = squared_errors.reduce(lambda x,y: x + y)\n    num_ratings = squared_errors.count()\n    return math.sqrt(total_error * 1.0 / num_ratings)\n\nranks = [4, 8, 12, 16]\nformatted_training = training_with_my_ratings.map(lambda x: (x[0], x[1], x[2])).cache()\nprint formatted_training.take(5)\nvalidation_for_predict = validation.map(lambda x: (x[0], x[1])).cache()\nvalidation_for_error = validation.map(lambda x: (x[0], x[1], x[2]))\n\n#task1 - train models for the training_with_my_ratings dataset\n#training_with_my_ratings\n\n#task2 - predict the ratings for the validation dataset and use compute error to find the error\nmin_error = float(\"inf\")\nbest_rank = -1\n\n\nfor rank in ranks:\n    model = ALS.train(formatted_training, rank)\n    predicted_ratings = model.predictAll(validation_for_predict)\n    error = compute_error(predicted_ratings, validation_for_error)\n    print rank, error\n    if error < min_error:\n        min_error = error\n        best_rank = rank\n\nprint \"The best model was trained with rank %s\" % best_rank",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[(1, 185, 5.0), (1, 231, 5.0), (1, 292, 5.0), (1, 316, 5.0), (1, 329, 5.0)]\n4"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 0.860899932125\n8"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 0.925873637346\n12"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 1.01071818294\n16"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 1.09064624097\nThe best model was trained with rank 4\n"
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "c) \n> __Which datasets did you decide to cache in memory and why?__ *\tThe datasets that were chosen to be cached in memory were validation_for_predict and the formatted_training  RDDs. these were chosen to be cached in memory as they are used in a for loop. They a cached to prevent the processor from having to go through the process of computing  them  on every increment of the loop control variable. *"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "d) Like you did in part 1, use the best model to predict the ratings on the `test` dataset, and compute the RMSE."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "test_for_predicting = test.map(lambda x: (x[0], x[1]))\nmodel = ALS.train(formatted_training, best_rank)\nprint model\npredicted_test = model.predictAll(test_for_predicting)\ntest_for_error = test.map(lambda x: (x[0], x[1], x[2]))\n\ntest_rmse = compute_error(test_for_error, predicted_test)\n\nprint \"The model had a RMSE on the test set of %s\" % test_rmse",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "<pyspark.mllib.recommendation.MatrixFactorizationModel object at 0x2709c50>\nThe model had a RMSE on the test set of 0.887247013288"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n"
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "e) Our solution code for part 1 got a RMSE of about 1.1 (subject to a small amount of variance, due to randomness in the ALS implementation) on the test set (you may want to use this to verify that your part 1 code was correct!).  How does this compare to the RMSE you got here?\n\n>__Is the test RMSE larger, smaller, or about the same as the RMSE (about 1.1) from part 1?  Why?__ The RMSE error for the test data was less than 1.1 for the test dataset. There was a difference between the RMSE of Part 1 of  0.233. The difference in the values is accounted to the fact that user inputs were taken into consideration in creating the model since the formatted_training RDD was inclusive of the user defined ratings."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Exercise 4: Recommending Movies for Yourself\n\na) So far, we've only used the `predictAll` method to compute the error of the model.  Here, use the `predictAll` to predict what ratings you'd give to the movies that you didn't already provide ratings for. Print out the 10 movies with the highest predicted ratings."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\n#skeleton to help\nformatted_test = test.map(lambda x: (x[0],x[1],x[2])).cache()\n\n\n\npredicted_10_highest_rated_movies = # YOUR CODE HERE\n\n# This should print a list of 10 movie names and the associated\n# predicted ratings.\nprint \"My highest rated movies: \", predicted_10_highest_rated_movies",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'predicted_10_highest_rated_movies' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-13-7b8744390745>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# This should print a list of 10 movie names and the associated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# predicted ratings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"My highest rated movies: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_10_highest_rated_movies\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'predicted_10_highest_rated_movies' is not defined"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "My highest rated movies: "
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "b)\n> __Are the predictions any good?  Do you think (or know!) you'd like these movies?__ Your answer here"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Wrapping Up\n\nPlease answer the questions in [this form](https://docs.google.com/a/berkeley.edu/forms/d/15i39zp0awFpNE36ljh58mctJ-PhDir0roS4VfPJHPvw/viewform) to help us improve the assignments for this class in the future.\n\n<p style=\"background-color: #f2dede; padding: 10px 15px; color: #b94a48;\">__Don't forget to submit your assignment using glookup!__ We will not automatically collect the notebooks on the cluster; you need to copy the notebook back to your local machine and submit it yourself when you're finished with the homework.  To save your iPython notebook to your machine from the cluster, go to \"File\" > \"Download as\" > \"iPython\".</p>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<a name=\"FAQ\"></a>\n## Frequently Asked Questions\n\n### How do I save my iPython notebook that's hosted in the cluster?\n\nGo to \"File\" > \"Download as\" > \"iPython\".\n\n### How do I get to the UI?\n\nThe UI for the Spark master is on port 8080, and the hostname is stored as part of the `CLUSTER_URL` variable pre-loaded into this notebook.  From the master UI, you can find the link to your application's UI.\n\n### What are all of the operations I can do on a Spark dataset (RDD)?\n\n[This Page](http://spark.apache.org/docs/0.9.0/api/pyspark/pyspark.rdd.RDD-class.html) lists all of the operations you can do on a Spark RDD.  Spark also has a Scala API (Scala is a programming language similar to Java); the [documentation for the Scala functions](http://spark.apache.org/docs/0.9.0/scala-programming-guide.html) is sometimes more helpful, and the Python functions work in the same way.\n\n### How do I use matplotlib?\n\nThere are lots of good examples on the [matplotlib website](http://matplotlib.org/index.html).  For example, [this page](http://matplotlib.org/examples/pylab_examples/simple_plot.html) shows how to plot a single line.\n\n### Why am I getting an OutOfMemoryError?\n\nIf you get an error that looks like: `org.apache.spark.SparkException: Job aborted: Exception while deserializing and fetching task: java.lang.OutOfMemoryError: Java heap space`, it probably means that you've tried to collect too much data on the machine where Python is running.  This is likely to happen if you do `collect()` on a large dataset.  To remedy this problem, you'll need to restart your iPython notebook (go to the main server, at port 8888 of the machine you were assigned, click \"Shutdown\" on your notebook, and then open it again) and don't do `collect()` on a large dataset.\n\nCurious why you're getting a Java error when your program is written in Python?  Spark is mostly written in Java (and Scala, a language built on top of Java).  We're using `pyspark` here, which uses a translation layer to translate between Python and Java.  Your Python `SparkContext` object is backed by a Java `SparkContext` object; all operations you run on Spark datasets are passsed through this Java object.  So, if you try to collect a result that's too large, the Java Virtual Machine that's running the Java `SparkContext` runs out of memory.\n\n### Python / Spark is giving me a crazy weird error!\n\nSpark is mostly written in Scala and Java, and the Python version of the code (\"pyspark\") hooks into the Java implementation in a way that can make error messages very difficult to understand.  If you get a hard-to-understand error when you run a Spark operation, we recommend first narrowing down the error so that you know exactly which operation caused the error.  For example, if `rdd.groupByKey().map(lambda x: x[1])` fails with an error, separate the `groupByKey()` and `map()` calls onto separate lines so you know which one is causing the error.  Next, double check the function signature to make sure you're passing the right arguments.  Pyspark can fail with a weird error if a RDD operation is given the wrong number or type of arguments.  If you're still stumped, try using `take(10)` to print out the first 10 entries in the dataset you're calling the RDD operation on.  Make sure the function you're calling and the arguments you're passing in make sense given the format of the input dataset.\n\n### I ran some code and nothing happened!\n\nAre you sure?  Some of the Spark operations will take a minute or so to run; look at the top of the iPython notebook to see if it says \"Kernel busy\".  If so, it's busy running your code.  Go checkout the Spark UI to see more about what's going on.\n\n<a name=\"faq_taking_forever\"></a>\n### My code is taking forever to run.  Did I do something wrong?\n\nYes.  In our solution code, none of the Spark stages take more than about a minute to run (remember that you can see a job's stages by looking at the Spark UI).  If you ran something and it's taking forever, double check that you passed in the datasets you inteded to.  One common problem from part 1 was that people passed RDDs in the wrong format (e.g., with 3 items instead of just a key and a value) to `join`.  Use `take()` to look at the datasets passed into the offending function to make sure they're in the right format.\n\nIf you end up with a job that's taking forever, you'll need to kill the job; otherwise it will hog all of your cluster resources such that you won't be able to run anything else.  Kill the job by shutting down this notebook (which will destroy your `SparkContext` and the associated worker process that's doing work) and then re-launching it.  You can shutdown your notebook by going to \"File\" > \"Close and Halt\" (but be sure to hit save first!).\n\n<a name=\"faq_no_predictions\"></a>\n### I'm not getting any predictions when I try to predict the ratings for myself!\n\nThe model will only provide predictions for users who were included in the training set.  Make sure you included your own data when you trained the model.\n\n<a name=\"faq_high_ratings\"></a>\n### Some of the predicted ratings are bigger than 5!  Did I do something wrong?\nThe ALS algorithm has no notion of bounds on expected ratings, so you may get predicted ratings that are larger than 5.  This does not necessarily signify a bug in your code."
    }
   ],
   "metadata": {}
  }
 ]
}