{
 "metadata": {
  "name": "hw3-part1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "CS194-16: Introduction to Data Science\n\n__Name:__ *Jonathan Manalus*\n\n__Student ID:__ *22037129*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Homework 3\n\n# Part 1: Predicting Movie Ratings\n\nOne of the most common uses of data is to predict what users want.  This allows Google to show you relevant ads, Amazon to recommend relevant products, and Netflix to recommend movies that you might like.  In this assignment, you'll explore how to recommend movies to a user.  We'll start with some basic methods, and then use machine learning to make more sophisticated predictions.\n\nWe'll use Spark for this assignment.  In part 1 of the assignment, you'll run Spark on your local machine and on a smaller dataset.  The purpose of this part of the assignment is to get everything working before adding the complexities of running on many machines.  The interface for running local Spark jobs is exactly the same as the interface for running jobs on a cluster, so you'll be using the same functions we used in lab, and all of the code you write locally can be executed on a cluster.  In part 2, which will be released after the midterm, you'll run Spark on a cluster that we have running for you (like in the lab).  You'll use the cluster to run your code on a larger dataset, and to predict which movies to recommend to yourself!\n\nAs mentioned during the lab, think carefully before calling `collect()` on any datasets.  When you're using a small, local dataset, calling `collect()` and then using Python to analyze the data locally will work fine, but this will not work when you're using a large dataset that doesn't fit in memory on one machine.  Solutions that call `collect()` and do local analysis that could have been done with Spark will not receive full credit.\n\nWe have created a [FAQ](#FAQ) at the bottom of this page (which is an expanded version of the FAQ from the lab) to help with common problems you may run into.  If you run into a problem, please check the FAQ before posting on Piazza!"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Exercise 0: Setup\n\na) As mentioned above, for this part of the assignment, you'll run Spark locally rather than on a cluster, for easier debugging.  Begin by downloading Spark from [this link](http://people.apache.org/~tdas/spark-0.9.1-rc3/spark-0.9.1-bin-cdh4.tgz).  Unzip and untar the file so you have a `spark-0.9.1-bin-cdh4` folder; this folder contains all of the code needed to run Spark.  We need to do a little bit of setup to tell iPython how to find Spark (we set this up for you on the cluster machines, but you need to do it yourself when running in your own VM).  We also need to start your own `SparkContext` (which we also did for you in the lab; the `SparkContext` was saved as `sc` in the lab).  The `SparkContext` is like a master for just your application.  It requests some resources from the cluster master, and it also breaks down jobs that you submit into stages of tasks.  For example, when you call `map()` on an resilient distributed dataset (RDD; Spark's name for datasets stored in memory), the `SparkContext` decides how many `map` tasks to run, and launches the `map` tasks on the executors allocated by the cluster master.\n\nFill in the path to the `spark` folder you just downloaded in the code below, and then execute it to create a `SparkContext` to use to run jobs."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Configure the necessary Spark environment.  pyspark needs SPARK_HOME setup\n# so it knows how to start the Spark master and some local workers for you to use.\nimport os\n# Fill this in with the path to the spark-0.9.1-bin-cdh4 folder you just downloaded\n# (e.g., /home/saasbook/spark-0.9.1-bin-cdh4)\npath_to_spark = './spark-0.9.1-bin-cdh4'\nos.environ['SPARK_HOME'] = path_to_spark\n\n# Set the python path so that we know where to find the pyspark files.\nimport sys\npath_to_pyspark = os.path.join(path_to_spark, \"python\")\nsys.path.insert(0, path_to_pyspark)\n\nfrom pyspark import SparkContext\n# You can set the app name to whatever you want; this just affects what\n# will show up in the UI.\napp_name = \"i<3datascience\"\nsc = SparkContext(\"local\", app_name)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Cannot run multiple SparkContexts at once",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-8-3a1cb6e4eccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# will show up in the UI.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mapp_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"i<3datascience\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapp_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/saasbook/Desktop/spark-0.9.1-bin-cdh4/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/saasbook/Desktop/spark-0.9.1-bin-cdh4/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance)\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot run multiple SparkContexts at once\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**Having trouble?  Checkout the [section in the FAQ](#faq_create_context) that covers issues creating a SparkContext.**\n\nEven though you're running Spark locally, Spark still starts the application web UI where you can see your application and what tasks it's running.  In a browser in the VM, go to [http://localhost:4040](http://localhost:4040/) to see the UI for your application.  There's no Master UI running here (the UI we saw at port 8080 during the lab) because Spark doesn't use a master when you run in local mode.\n\nb) Next, download the datafiles that you'll need for the assignment from [https://github.com/amplab/datascience-sp14/raw/master/hw3/part1files.tar.gz](https://github.com/amplab/datascience-sp14/raw/master/hw3/part1files.tar.gz).  You'll do all of your analysis on the `ratings.dat` and `movies.dat` datasets located in the `part1files` folder that you just downloaded.  These are smaller versions of the datasets we used in lab 8.  As in the lab, each entry in the ratings dataset is formatted as `UserID::MovieID::Rating::Timestamp` and each entry in the movies dataset is formatted as `MovieID::Title::Genres`.  Read these two datasets into memory.  You can count the number of entries in each dataset to ensure that you've loaded them correctly; the ratings dataset should have 100K entries and the movies dataset should have 1682 entries.\n\nNote that when you create a new dataset using `sc.textFile`, you can give an absolute path to the dataset on your filesystem, e.g. `/Users/kay/part1files/ratings.dat'."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "### YOUR CODE HERE\nraw_ratings = sc.textFile(\"part1files/ratings.dat\", 10)\nraw_ratings.setName(\"raw ratings\")\nraw_ratings.cache()\n\nraw_movies = sc.textFile(\"part1files/movies.dat\", 10)\nraw_movies.setName(\"raw movies\")\nraw_movies.cache()\n\nratings_count = raw_ratings.count()\nmovies_count = raw_movies.count()\nprint \"%s ratings and %s movies in the datasets\" % (ratings_count, movies_count)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "100000 ratings and 1682 movies in the datasets\n"
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Exercise 1: Basic Recommendations\n\na) One way to recommend movies is to always recommend the movies with the highest average rating.  Use Spark to find the name and the average rating of the 5 movies with the highest average rating."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#convert data type to tuple\ndef get_tuple(entry):\n    items = entry.split(\"::\")\n    return int(items[0]), int(items[1]), float(items[2]), int(items[3])\n    #items[0] = user id, items[1]= movie id, items[2] = rating and items[3]= timestamp\n    \ndef make_tuple(entry):\n    items = entry.split(\"::\")\n    movieID = int(items[0])\n    movie_title = items[1]\n    genres = items[2].split(\"|\")\n    return (movieID, movie_title, genres)\n    #items[0]= movie ID, items[1] = movie name and year, items[2]=genres\n\nratings = raw_ratings.map(get_tuple)\nratings.setName(\"Ratings\")\nratings.cache()\nnum_ratings = ratings.count()\n\nmovieList = raw_movies.map(make_tuple)\nmovieList.setName(\"Movies\")\nmovieList.cache()\nnum_movie = movieList.count()\n\ndef add_two_counts(count1, count2):\n    return count1 + count2\n\n#get movies and ratings in one tuple\n#get movie ids and names\nmovie_ids_and_titles = movieList.map(lambda x: (x[0], x[1]))\n\n#movie_ids_and_titles.cache()\n#print movie_ids_and_titles.take(10)  #uncomment to see result of RDD transformation\n\n#get ratings  and movie ids\nratings_id_and_value = ratings.map(lambda x: (x[1],x[2]))\nratings_id_and_value.cache()\n#print \"Movie Id and corresponding rating \",ratings_id_and_value.take(10) #Uncomment to print out the first ten values\n\n#all rating for each movie RDD\nmovie_id_all_ratings = ratings_id_and_value.groupByKey(5)\nmovie_id_all_ratings.cache()\n#print \"First five movies and all the given ratings\",movie_id_all_ratings.take(5)  #uncomment to see result of RDD transformation\n\n#want to count the elements of index 1 of RDD movie_id_all_ratings then make a new data set with the RDD movies_and_sum_of_ratings\n\n\n#Movies and sums of ratings\nmovies_and_sum_of_ratings = movie_id_all_ratings.map(lambda x: (x[0],sum(x[1])))\nmovies_and_sum_of_ratings.cache()\n#print \"Movie ID and Total movie ratings\", movies_and_sum_of_ratings.take(5)   #uncomment to see result of RDD transformation\n\n\n#count = len(movie_id_all_ratings[0][1]) \ncount = movie_id_all_ratings.map(lambda x: (x[0],len(x[1])))    #Prints out a count of how many ratings each movie has.\ncount.cache()\n#print count.take(5)   #uncomment to see result of RDD transformation\n#now join count with movie_id_all_ratings to make  new RDD then find average rating for each movie by using a little algebra.\n\n\nmovieId_sum_of_ratings_and_ratings_count = movies_and_sum_of_ratings.map(lambda x: (x[0],x[1])).join(count)\nmovieId_sum_of_ratings_and_ratings_count.cache()\n#print \"First 10 movies with sum of ratings and count or ratings\",movieId_sum_of_ratings_and_ratings_count.take(10) \n\n\nmovie_and_average_rating = movieId_sum_of_ratings_and_ratings_count.map(lambda x: (x[0],(x[1][0]/x[1][1])))\nmovie_and_average_rating.cache()\n#print\"Movie titles and average ratings\", movie_and_average_rating.take(5)  #uncomment to see result of RDD transformation\n\n#movie titles and summed ratings\nmovie_titles_and_sum_ratings = movie_ids_and_titles.map(lambda x: (x[0],x[1])).join(count)\n#print \"Movie titles and summed ratings\",movie_titles_and_sum_ratings.take(10) \n\n\nmovieNames_and_average_ratings = movie_ids_and_titles.map(lambda x: (x[0],x[1])).join(movie_and_average_rating)\n#print \"Movies by title and average ratings\",movieNames_and_average_ratings.take(5) #uncomment to see result of RDD transformation\n\n#EXERCISE 1.0A UPDATED\n#highest_rated_movies\nhighest_rated_movies = movieNames_and_average_ratings.map(lambda x:  (x[1][1], (x[1][0],x[0]))).sortByKey(False)\nprint \"5 highest rated movies (and associated average ratings): \", highest_rated_movies.take(5)\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "5 highest rated movies (and associated average ratings):  "
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[(5.0, (u'Great Day in Harlem, A (1994)', 814)), (5.0, (u'They Made Me a Criminal (1939)', 1122)), (5.0, (u'Prefontaine (1997)', 1189)), (5.0, (u'Marlene Dietrich: Shadow and Light (1996) ', 1201)), (5.0, (u'Star Kid (1997)', 1293))]\n"
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "b) The movies you found may seem a bit suspicious.  How many ratings does each of those movies have?"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#map highest rated movies with count\n#firstly remap movie names with count\n#movie_id_and_sum_of_ratings = movieId_sum_of_ratings_and_ratings_count.map(lambda x:(x[0],x[1])).join(count)\n#print \"Movie id and sum amount of ratings received\", movie_id_and_sum_of_ratings.take(5)\n\ndef getMovieIDandAverageRatingTuple(HRM):\n    movieId = HRM[1][1]\n    movieAverageRating = HRM[0]\n    return (movieId,movieAverageRating)\n\nmovieId_and_highest_average_rated = highest_rated_movies.take(5)\nmovieId_and_highest_average_rated = sc.parallelize(movieId_and_highest_average_rated).map(getMovieIDandAverageRatingTuple)\n#print movieId_and_highest_average_rated.collect()\n\n#print \"\\n\"\n\ndef getMovieIDandCountOfRatings(dataset):\n    movieId = dataset[0]\n    count_of_ratings = dataset[1][1]\n    return (movieId, count_of_ratings)\n\nmovieId_and_count_of_ratings = movieId_sum_of_ratings_and_ratings_count.map(getMovieIDandCountOfRatings)\n#print movieId_and_count_of_ratings.take(10)\n\n#print \"\\n\"\nfalseHighest = movieId_and_highest_average_rated.sortByKey(True)\n#print falseHighest.take(10)\n\nnewRDD = falseHighest.join(movieId_and_count_of_ratings)\n#print newRDD.take(10)\n\n\nhighest_rated_movies_rating_counts = newRDD.join(movie_ids_and_titles).take(10)\nprint \"Number of ratings for each of the 5 highest rated movies: \", highest_rated_movies_rating_counts",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Number of ratings for each of the 5 highest rated movies:  [(814, ((5.0, 1), u'Great Day in Harlem, A (1994)')), (1122, ((5.0, 1), u'They Made Me a Criminal (1939)')), (1189, ((5.0, 3), u'Prefontaine (1997)')), (1201, ((5.0, 1), u'Marlene Dietrich: Shadow and Light (1996) ')), (1293, ((5.0, 3), u'Star Kid (1997)'))]\n"
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "c) How can you improve your recommendations?  Improve upon your recommendations in part (a) to recommend 5 movies that you expect to be well-liked.  You are not expected to use any sophisticated machine learning techniques here; using just the Spark operations we learned in lab is sufficient."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "### YOUR CODE HERE\nmovieTitles_and_ratingsCount = movie_ids_and_titles.map(lambda x: (x[0],x[1])).join(count)\nnuffViews = movieTitles_and_ratingsCount.filter(lambda x: x[1][1] >= 300)\n\n#map nuffViews back to average rating \n#return movie name and average rating if average rating > 4.0\n\nmost_viewed_and_average_ratings = nuffViews.map(lambda x: (x[0],x[1][0])).join(movieNames_and_average_ratings)\n\ndef make_recommended_movies_tuple(entry):\n    movieId = entry[0]\n    movieTitle = entry[1][0]\n    avgRating = entry[1][1][1]\n    return (int(movieId),movieTitle,avgRating)\n\nrecommended_movie_tuple = most_viewed_and_average_ratings.map(make_recommended_movies_tuple)\n    \n\nrecommended_movies = recommended_movie_tuple.filter(lambda x: x[2] > 4)\nprint \"5 recommended movies are: \", recommended_movies.take(5)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "5 recommended movies are:  "
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[(50, u'Star Wars (1977)', 4.3584905660377355), (56, u'Pulp Fiction (1994)', 4.060913705583756), (79, u'Fugitive, The (1993)', 4.044642857142857), (98, u'Silence of the Lambs, The (1991)', 4.28974358974359), (100, u'Fargo (1996)', 4.155511811023622)]\n"
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": ">__Describe how you improved on the recommendations in part (a) in at most 4 sentences.__ *Basically a better way of selecting a high rated movie would be to select the movies which have the highest number of ratings ,and a minimum average rating of 4. This would ensure the highly rated movies are also amongst the most view, since currently in 1A there is the fact that a single user rated movie with a rating of 5.0 would come up as a top rated movie which doesn't mean it's should be. *"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Exercise 2: Collaborative Filtering\n\nYou've learned about many of the basic transformations and actions that Spark allows us to do on distributed datasets.  Spark also exposes some higher level functionality; in particular, machine learning using a component of Spark called MLlib.  In this assignment, you'll use MLlib to make personalized movie recommendations for you using the movie data we've been analyzing.\n\nWe're going to use a technique called collaborative filtering.  The basic idea of collaborative filtering is that we start with a matrix whose entries are movie ratings.  Each row represents a user and each column represents a particular movie (shown in red in the diagram below).\n\nWe don't know all of the entries in this matrix, which is precisely why we need collaborative filtering.  For each user, we have ratings for only a subset of the movies.  With collaborative filtering, the idea is to approximate the ratings matrix by factorizing it as the product of two matrices: one that describes properties of each user (shown in green), and one that describes properties of each movie (shown in blue).\n\n![factorization](http://ampcamp.berkeley.edu/big-data-mini-course/img/matrix_factorization.png)\n\nWe want to select these two matrices such that the error for the users/movie pairs where we know the correct ratings is minimized.  The ALS (alternating least squares) algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the movies matrix constrant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name.\n\nThis optimization is what's being shown on the right in the image above.  Given a fixed set of user factors (i.e., values in the users matrix), we use the known ratings to find the best values for the movie factors using the optimization written at the bottom of the figure.  Then we \"alternate\" and pick the best user factors given fixed movie factors.\n\nFor a simple example of what the users and movies matrices might look like, check out the [slides from lecture 9](http://goo.gl/l4Stce).\n\na) Before jumping into the machine learning, you need to break up the dataset into a training set (which we'll use to train models), a validation set (which we'll use to choose the best model), and a test set.  One way that people often partition data is using the time stamp: using the least significant digit of the timestamp is an essentially random way to split the dataset into multiple groups.  Use the least significant digit of the rating timestamp to separate 60% of the data into a training set, 20% into a validation set, and the remaining 20% into a test set."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "entries_with_timestampLSD_3 = ratings.filter(lambda x: x[3]%10)\n#print entries_with_timestampLSD_3.collect()\n\nsixty = int(num_ratings*0.6)\ntwenty = int(num_ratings*0.2)\n\nValidation = entries_with_timestampLSD_3.take(twenty)\nvalidation = len(Validation)\n\nTest = entries_with_timestampLSD_3.take(twenty)\ntest = len(Test)\n\nTraining = entries_with_timestampLSD_3.take(sixty)\ntraining = len(Training)\n\nvalidation = sc.parallelize(Validation)\ntest = sc.parallelize(Test)\ntraining = sc.parallelize(Training)\n\nprint \"Training: %s, validation: %s, test: %s\" % (training.count(), validation.count(), test.count())",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Training: 60000, validation: 20000, test: 20000\n"
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "After splitting the dataset, your training set should have about 60K entries and the validation and test sets should have about 20K entries (the exact number of entries in each dataset will vary slightly depending on the method you used to split the data into the 3 sets).\n\nb) In the next part, you'll generate a few different models, and will need a way to decide which model is best. We'll use the root mean squared error (RMSE) to compute the error of each model.  The root mean squared error is the square root of the average value of `(actual rating - predicted rating)^2` for all users and movies for which we have the actual rating."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": ">__If your model perfectly predicts the user ratings, what will the root mean squared error be?__ *If the model perfectly predicts the user ratings then the result of \n(actual-predicted)^2 will be zero (0) and thus the resulting RMS Error will be 0. *\n\n>__If all of the predicted ratings are off by one (they're 1 higher or lower than the actual ratings), what will the RMSE be?__ *The model perfectly predicts the user ratings then the result of (actual - predicted)^2 will always be 1, since +1 or -1 squared is 1. The sum will then return the count of the the number of key value pairs. The RMS Error will therefore be 1, and Root means Squared is calculated using the formula :Where in our case:\nYj is the actual rating and \nYj^ is the predicted rating.\nn is the number of (actual, predicted) pairs.*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "c) Write a function to compute the sum of squared error given a `predicted` and `actual` RDD."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Hint: you may want to use the math module to compute the square root!\nimport math\n\ndef getUserIDMovieIDandPredictedRating(entry):\n    user = entry[0]\n    movie = entry[1]\n    predicted = entry[2]\n    return (user,movie,predicted)\n\ndef getUserIDMovieIDandActualRating(entry):\n    user = entry[0]\n    movie = entry[1]\n    actual = entry[2]\n    return (user,movie,actual)\n\npredictedList = test_predicted.map(getUserIDMovieIDandPredictedRating).collect()\nprint \"Predicted List \", predictedList\n\nactualList = test_actual.map(getUserIDMovieIDandActualRating).collect()\nprint \"\\n\"\nprint \"Actual List\", actualList\n\n\n\ndef compute_error(predicted, actual):\n    #\"\"\" Compute the root mean squared error between predicted and actual.\n    \n    #Params:\n     # predicted: An RDD of predicted ratings for each movie and each user where each entry is in the form (user, movie, rating).\n    # actual: An RDD of actual ratings where each entry is in the form (user, movie, rating).\n    #\"\"\"\n    ### YOUR CODE HERE\n    #turn RDD back into  a list since it is easier to manipulate list\n    test_predicted_list = test_predicted.collect()\n    test_actual_list = test_actual.collect() #Note that this is a list of all the (userID, movieID, ratings) tuples thus\n    #test_actual_list[0] is an tuple and test_actual_list[0][0] is the userID of the first element (userID, movieID, rating)\n    \n    predictedList = test_predicted.map(getUserIDMovieIDandPredictedRating).collect()\n    actualList = test_actual.map(getUserIDMovieIDandActualRating).collect()\n    \n    \n    #only apply subtraction if movie ID is the same for both datasets\n    if (test_actual_list[0][1]==test_predicted_list[0][1]):\n        difference = test_actual_list[0][2] - test_predicted_list[0][2]\n        square_difference = difference**2\n        average = sum(test_predicted_list[0])/len(test_actual_list)\n        rms_error = math.sqrt(square_difference/average)\n    return rms_error\n    \n    \n\n# sc.parallelize turns a Python list into a Spark RDD.\ntest_predicted = sc.parallelize([\n    (1, 1, 5),\n    (1, 2, 3),\n    (1, 3, 4),\n    (2, 1, 3),\n    (2, 2, 2),\n    (2, 3, 4)])\ntest_actual = sc.parallelize([\n     (1, 2, 3),\n     (1, 3, 5),\n     (2, 1, 5),\n     (2, 2, 1)])\n# The error for the test datasets should be 1.2247\nprint \"Error for test datasets: %s\" % compute_error(test_predicted, test_actual)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'test_predicted' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-14-6310fd729001>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmovie\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mpredictedList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_predicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetUserIDMovieIDandPredictedRating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Predicted List \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictedList\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'test_predicted' is not defined"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": "Exception AttributeError: \"'SparkContext' object has no attribute '_jsc'\" in <bound method SparkContext.__del__ of <pyspark.context.SparkContext object at 0xae36d8c>> ignored\n"
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "d) In this part, we'll use `ALS.train` to train a bunch of models, and we'll select the best model.  The most important parameter to ALS is the rank, which is the number of columns in the Users matrix (green in the diagram above) or the number of rows in the Movies matrix.  In general, a lower rank will mean higher error on the training dataset, but a high rank may lead to overfitting.  Train models with ranks of 4, 8, 12, and 16 using the `training` dataset, predict the ratings for the `validation` dataset, and use the `compute_error` function you wrote in part `(b)` to compute the error.  Which rank produces the best model, based on the error on the `validation` dataset?  Note that the values here will be more meaningful when we run on a cluster with a larger dataset.\n\nTo create the model, use `ALS.train(training_rdd, rank)`, which takes two parameters: an RDD in the format (user, movie, rating) to use to train the model, and an integer rank.  To predict rating values, call `model.predictAll` with the `validation` dataset, where `model` is the model generated with `ALS.train`.  `predictAll` accepts an RDD with each entry in the format (user, movie) and outputs an RDD with each entry in the format (user, movie, rating)."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from pyspark.mllib.recommendation import ALS\n\nbest_rank = # YOUR CODE HERE\n\nprint \"The best model was trained with rank %s\" % best_rank",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "e) So far, we used the `training` and `validation` datasets to select the best model.  Since we used these two datasets to determine what model is best, we can't use them to test how good the model is (otherwise we'd be vulnerable to overfitting).  To decide how good our model is, we need to use the `test` dataset.  Use the model you created in part (c) to predict the ratings for the test dataset and compute the RMSE."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "test_rmse = # YOUR CODE HERE\n\nprint \"The model had a RMSE on the test set of %s\" % test_rmse",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "You now have code to predict how users will rate movies!  In the next part of the assignment, you'll run this code on a larger dataset using a cluster of machines, like we did in the lab.  You'll use the larger dataset to generate a better model, and then will use that model to predict what movies to recommend to yourself.  Until then, good luck on the midterm!"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<a name=\"FAQ\"></a>\n## Frequently Asked Questions\n\n### I'm not sure where to start.  Some parts of this homework seem impossible to do with the functions we learned about in lab!\n\nWith the exception of the ML functions that we introduce in this assignment, you should be able to complete all parts of this homework using only the functions we learned in lab (although you're welcome to use more features of Spark if you like!).  You may need to use the functions introduced at the end of the lab (which we didn't get to in class); if you're having trouble, try walking through the last sections of the lab to see if they help.\n\n### How do I get to the UI?\n\nThe UI for your application is on port 4040 on your local machine (so go to [http://localhost:4040](http://localhost:4040)).  There is no master web UI when you run locally because there is no Spark master -- just a single worker process to run your tasks.\n\n### What are all of the operations I can do on a Spark dataset (RDD)?\n\nIf you click the \"RDD\" link on [this Page](http://spark.apache.org/docs/0.9.0/api/pyspark/index.html), it lists all of the operations you can do on a Spark RDD.  Spark also has a Scala API (Scala is a programming language similar to Java); the [documentation for the Scala functions](http://spark.apache.org/docs/0.9.0/scala-programming-guide.html) is sometimes more helpful, and the Python functions work in the same way.\n\n### How do I use matplotlib?\n\nThere are lots of good examples on the [matplotlib website](http://matplotlib.org/index.html).  For example, [this page](http://matplotlib.org/examples/pylab_examples/simple_plot.html) shows how to plot a single line.\n\n### Why am I getting an OutOfMemoryError?\n\nIf you get an error that looks like: `org.apache.spark.SparkException: Job aborted: Exception while deserializing and fetching task: java.lang.OutOfMemoryError: Java heap space`, it probably means that you've tried to collect too much data on the machine where Python is running.  This is likely to happen if you do `collect()` on a large dataset.  The best way to remedy this problem is to restart your iPython notebook (go to the main server, at port 8888 of the machine you were assigned, click \"Shutdown\" on your notebook, and then open it again) and don't do `collect()` on a large dataset.\n\nCurious why you're getting a Java error when your program is written in Python?  Spark is mostly written in Java (and Scala, a language built on top of Java).  We're using `pyspark` here, which uses a translation layer to translate between Python and Java.  Your Python `SparkContext` object is backed by a Java `SparkContext` object; all operations you run on Spark datasets are passsed through this Java object.  So, if you try to collect a result that's too large, the Java Virtual Machine that's running the Java `SparkContext` runs out of memory.\n\n### Python / Spark is giving me a crazy weird error!\n\nSpark is mostly written in Scala and Java, and the Python version of the code (\"pyspark\") hooks into the Java implementation in a way that can make error messages very difficult to understand.  If you get a hard-to-understand error when you run a Spark operation, we recommend first narrowing down the error so that you know exactly which operation caused the error.  For example, if `rdd.groupByKey().map(lambda x: x[1])` fails with an error, separate the `groupByKey()` and `map()` calls onto separate lines so you know which one is causing the error.  Next, double check the function signature to make sure you're passing the right arguments.  Pyspark can fail with a weird error if a RDD operation is given the wrong number or type of arguments.  If you're still stumped, try using `take(10)` to print out the first 10 entries in the dataset you're calling the RDD operation on.  Make sure the function you're calling and the arguments you're passing in make sense given the format of the input dataset.\n\n### I ran some code and nothing happened!\n\nAre you sure?  Some of the Spark operations will take a minute or so to run; look at the top of the iPython notebook to see if it says \"Kernel busy\".  If so, it's busy running your code.  Go checkout the Spark UI to see more about what's going on.\n\n### My code is taking forever to run.  Did I do something wrong?\n\nProbably.  In our solution code, none of the Spark jobs take more than a minute to run.  If you ran something and it's taking forever, double check that you passed in the datasets you inteded to.  If all else failed, create a small sample dataset and try your code on that to make sure things are working.\n\nIf you end up with a job that's taking forever, you can kill the job by shutting down this notebook (which will destroy your `SparkContext` and the associated worker process that's doing work) and then re-launching it.  Be sure to hit save first!\n\n<a name=\"faq_create_context\"></a>\n### I'm having trouble creating `SparkContext`...\n\n#### I'm getting an error that says \"`Exception AttributeError: \"'SparkContext' object has no attribute '_jsc'\"`\".\n\nWhen you try to create a `SparkContext`, you may get an error that ends with a red box with text that looks like: `Exception AttributeError: \"'SparkContext' object has no attribute '_jsc'\" in <bound method SparkContext.__del__ of <pyspark.context.SparkContext object at 0x10c876890>> ignored`.  This is a benign error that happens when the Spark Context tries to shut down,  but it signals that there was an error when creating the SparkContext.  Look at the error messages above this one to see what the real problem is.\n\n#### I'm getting an error that says \"`ImportError: No module named pyspark`\"\n\nThis means that you didn't give the correct path to Spark when setting the `path_to_spark` variable.  Ensure that the path listed here matches the path to the Spark folder you downloaded.  When you change to the correct path, you may need to shutdown and restart your notebook for all of the path setup to work correctly again.\n\n#### I'm getting an error that says \"`ValueError: Cannot run multiple SparkContexts at once`\"\n\nYou've created multiple `SparkContext`s, likely by executing the code to create a new Spark Context multiple times.  Either (a) use the `SparkContext` that you created earlier or (b) shutdown your notebook, restart it, and then re-run the relevant code only a single time.\n\n<a name=\"flat_map\"></a>\n### Can you explain more about how `flatMap()` works?\n\nLet's look at an example: suppose you have an RDD where each entry lines of text in a book, and you want to make a new RDD where each entry is a single word.  You could use `flatMap()` to do this as follows:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "lines_in_book = [\n  \"I am Sam\",\n  \"I am Sam\",\n  \"Sam I am\",\n  \"Do you like\",\n  \"green eggs and ham?\"]\n# sc.parallelize turns a Python list into an RDD.\nlines_in_book_rdd = sc.parallelize(lines_in_book)\n\n# Notice that here, the function passed to flat map will return a list.\nwords_rdd = lines_in_book_rdd.flatMap(lambda x: x.split(\" \"))\n\nprint words_rdd.collect()",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The resulting RDD will have a list of words. The function we passed into `flatMap` returned a list of words for each entry in the original RDD, and `flatMap` combines all of these lists of words into a single list.  Let's do this same thing with `map` to see what's different."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "list_of_words_rdd = lines_in_book_rdd.map(lambda x: x.split(\" \"))\nprint lines_of_words_rdd.collect()",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Notice that now the resulting RDD has a list of lists.\n\nAnother way to think about this is that `map()` always returns a new RDD with the same number of entries as the original RDD: each entry in the original RDD is mapped to one entry in the new RDD.  With `flatMap()`, each entry in the original RDD maps to a list of 0 or more entries, so the new RDD isn't necessarily the same size as the old RDD (it might be larger or smaller)."
    }
   ],
   "metadata": {}
  }
 ]
}